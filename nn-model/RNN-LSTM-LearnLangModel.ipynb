{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import utils, vocabulary, rnnlm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 98552 sentences (2.62178e+06 tokens)\n",
      "Training set: 78841 sentences (2089056 tokens)\n",
      "Test set: 19711 sentences (532729 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# assert(nltk.download('gutenberg'))\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"gutenberg\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10000)\n",
      "(?, ?, 1)\n"
     ]
    }
   ],
   "source": [
    "TF_GRAPHDIR = \"tf_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = False\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_: w, lm.target_y_: y, lm.initial_h_: h}\n",
    "        ## print \"Calling computational graph\"\n",
    "        cost, train_op_ = session.run([loss, train_op], feed_dict)\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "        \n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=100, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 63]: seen 64000 words at 6345 wps, loss = 6.477\n",
      "[batch 133]: seen 134000 words at 6659 wps, loss = 5.980\n",
      "[batch 200]: seen 201000 words at 6659 wps, loss = 5.707\n",
      "[batch 270]: seen 271000 words at 6726 wps, loss = 5.513\n",
      "[batch 340]: seen 341000 words at 6771 wps, loss = 5.366\n",
      "[batch 409]: seen 410000 words at 6791 wps, loss = 5.258\n",
      "[batch 479]: seen 480000 words at 6808 wps, loss = 5.170\n",
      "[batch 549]: seen 550000 words at 6821 wps, loss = 5.094\n",
      "[batch 619]: seen 620000 words at 6832 wps, loss = 5.027\n",
      "[batch 688]: seen 689000 words at 6838 wps, loss = 4.970\n",
      "[batch 757]: seen 758000 words at 6843 wps, loss = 4.922\n",
      "[batch 827]: seen 828000 words at 6852 wps, loss = 4.877\n",
      "[batch 897]: seen 898000 words at 6856 wps, loss = 4.837\n",
      "[batch 965]: seen 966000 words at 6850 wps, loss = 4.801\n",
      "[batch 1036]: seen 1037000 words at 6862 wps, loss = 4.770\n",
      "[batch 1106]: seen 1107000 words at 6868 wps, loss = 4.739\n",
      "[batch 1176]: seen 1177000 words at 6875 wps, loss = 4.710\n",
      "[batch 1246]: seen 1247000 words at 6878 wps, loss = 4.685\n",
      "[batch 1316]: seen 1317000 words at 6881 wps, loss = 4.661\n",
      "[batch 1385]: seen 1386000 words at 6881 wps, loss = 4.639\n",
      "[batch 1455]: seen 1456000 words at 6882 wps, loss = 4.617\n",
      "[batch 1525]: seen 1526000 words at 6884 wps, loss = 4.597\n",
      "[batch 1594]: seen 1595000 words at 6883 wps, loss = 4.578\n",
      "[batch 1663]: seen 1664000 words at 6880 wps, loss = 4.560\n",
      "[batch 1732]: seen 1733000 words at 6877 wps, loss = 4.542\n",
      "[batch 1802]: seen 1803000 words at 6880 wps, loss = 4.526\n",
      "[batch 1871]: seen 1872000 words at 6880 wps, loss = 4.511\n",
      "[batch 1941]: seen 1942000 words at 6881 wps, loss = 4.496\n",
      "[batch 2009]: seen 2010000 words at 6877 wps, loss = 4.483\n",
      "[batch 2078]: seen 2079000 words at 6878 wps, loss = 4.469\n",
      "[batch 2149]: seen 2150000 words at 6884 wps, loss = 4.455\n",
      "[batch 2217]: seen 2218000 words at 6880 wps, loss = 4.443\n",
      "[epoch 1] Completed in 0:05:26\n",
      "[epoch 1] Train set: avg. loss: 5.230  (perplexity: 186.81)\n",
      "[epoch 1] Test set: avg. loss: 5.233  (perplexity: 187.39)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "[batch 68]: seen 69000 words at 6859 wps, loss = 4.024\n",
      "[batch 138]: seen 139000 words at 6882 wps, loss = 4.015\n",
      "[batch 207]: seen 208000 words at 6880 wps, loss = 4.019\n",
      "[batch 275]: seen 276000 words at 6850 wps, loss = 4.013\n",
      "[batch 345]: seen 346000 words at 6864 wps, loss = 4.003\n",
      "[batch 415]: seen 416000 words at 6876 wps, loss = 4.003\n",
      "[batch 486]: seen 487000 words at 6895 wps, loss = 3.999\n",
      "[batch 555]: seen 556000 words at 6895 wps, loss = 3.994\n",
      "[batch 623]: seen 624000 words at 6884 wps, loss = 3.990\n",
      "[batch 692]: seen 693000 words at 6878 wps, loss = 3.983\n",
      "[batch 761]: seen 762000 words at 6874 wps, loss = 3.980\n",
      "[batch 830]: seen 831000 words at 6871 wps, loss = 3.976\n",
      "[batch 900]: seen 901000 words at 6877 wps, loss = 3.972\n",
      "[batch 969]: seen 970000 words at 6877 wps, loss = 3.968\n",
      "[batch 1038]: seen 1039000 words at 6873 wps, loss = 3.966\n",
      "[batch 1105]: seen 1106000 words at 6857 wps, loss = 3.962\n",
      "[batch 1175]: seen 1176000 words at 6863 wps, loss = 3.957\n",
      "[batch 1245]: seen 1246000 words at 6870 wps, loss = 3.955\n",
      "[batch 1316]: seen 1317000 words at 6881 wps, loss = 3.952\n",
      "[batch 1386]: seen 1387000 words at 6886 wps, loss = 3.949\n",
      "[batch 1456]: seen 1457000 words at 6887 wps, loss = 3.946\n",
      "[batch 1526]: seen 1527000 words at 6888 wps, loss = 3.942\n",
      "[batch 1596]: seen 1597000 words at 6891 wps, loss = 3.939\n",
      "[batch 1666]: seen 1667000 words at 6893 wps, loss = 3.936\n",
      "[batch 1737]: seen 1738000 words at 6898 wps, loss = 3.933\n",
      "[batch 1806]: seen 1807000 words at 6897 wps, loss = 3.930\n",
      "[batch 1876]: seen 1877000 words at 6899 wps, loss = 3.928\n",
      "[batch 1946]: seen 1947000 words at 6899 wps, loss = 3.925\n",
      "[batch 2015]: seen 2016000 words at 6897 wps, loss = 3.923\n",
      "[batch 2085]: seen 2086000 words at 6898 wps, loss = 3.920\n",
      "[batch 2153]: seen 2154000 words at 6895 wps, loss = 3.917\n",
      "[batch 2223]: seen 2224000 words at 6896 wps, loss = 3.915\n",
      "[epoch 2] Completed in 0:05:25\n",
      "[epoch 2] Train set: avg. loss: 5.011  (perplexity: 149.99)\n",
      "[epoch 2] Test set: avg. loss: 5.016  (perplexity: 150.77)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "[batch 70]: seen 71000 words at 6997 wps, loss = 3.815\n",
      "[batch 140]: seen 141000 words at 6953 wps, loss = 3.809\n",
      "[batch 210]: seen 211000 words at 6936 wps, loss = 3.814\n",
      "[batch 280]: seen 281000 words at 6936 wps, loss = 3.810\n",
      "[batch 349]: seen 350000 words at 6920 wps, loss = 3.805\n",
      "[batch 418]: seen 419000 words at 6916 wps, loss = 3.807\n",
      "[batch 488]: seen 489000 words at 6916 wps, loss = 3.806\n",
      "[batch 558]: seen 559000 words at 6918 wps, loss = 3.802\n",
      "[batch 628]: seen 629000 words at 6916 wps, loss = 3.800\n",
      "[batch 698]: seen 699000 words at 6921 wps, loss = 3.795\n",
      "[batch 768]: seen 769000 words at 6922 wps, loss = 3.795\n",
      "[batch 838]: seen 839000 words at 6921 wps, loss = 3.794\n",
      "[batch 908]: seen 909000 words at 6921 wps, loss = 3.791\n",
      "[batch 979]: seen 980000 words at 6928 wps, loss = 3.789\n",
      "[batch 1050]: seen 1051000 words at 6933 wps, loss = 3.787\n",
      "[batch 1120]: seen 1121000 words at 6935 wps, loss = 3.785\n",
      "[batch 1189]: seen 1190000 words at 6929 wps, loss = 3.782\n",
      "[batch 1260]: seen 1261000 words at 6935 wps, loss = 3.781\n",
      "[batch 1330]: seen 1331000 words at 6936 wps, loss = 3.780\n",
      "[batch 1401]: seen 1402000 words at 6940 wps, loss = 3.779\n",
      "[batch 1473]: seen 1474000 words at 6951 wps, loss = 3.777\n",
      "[batch 1543]: seen 1544000 words at 6952 wps, loss = 3.775\n",
      "[batch 1614]: seen 1615000 words at 6957 wps, loss = 3.773\n",
      "[batch 1684]: seen 1685000 words at 6958 wps, loss = 3.771\n",
      "[batch 1754]: seen 1755000 words at 6956 wps, loss = 3.770\n",
      "[batch 1824]: seen 1825000 words at 6954 wps, loss = 3.769\n",
      "[batch 1893]: seen 1894000 words at 6952 wps, loss = 3.768\n",
      "[batch 1963]: seen 1964000 words at 6953 wps, loss = 3.767\n",
      "[batch 2032]: seen 2033000 words at 6949 wps, loss = 3.766\n",
      "[batch 2100]: seen 2101000 words at 6944 wps, loss = 3.764\n",
      "[batch 2170]: seen 2171000 words at 6943 wps, loss = 3.763\n",
      "[batch 2241]: seen 2242000 words at 6945 wps, loss = 3.762\n",
      "[epoch 3] Completed in 0:05:23\n",
      "[epoch 3] Train set: avg. loss: 4.898  (perplexity: 134.01)\n",
      "[epoch 3] Test set: avg. loss: 4.905  (perplexity: 134.91)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "[batch 67]: seen 68000 words at 6791 wps, loss = 3.703\n",
      "[batch 136]: seen 137000 words at 6838 wps, loss = 3.696\n",
      "[batch 205]: seen 206000 words at 6844 wps, loss = 3.702\n",
      "[batch 274]: seen 275000 words at 6857 wps, loss = 3.698\n",
      "[batch 344]: seen 345000 words at 6876 wps, loss = 3.692\n",
      "[batch 413]: seen 414000 words at 6875 wps, loss = 3.695\n",
      "[batch 479]: seen 480000 words at 6833 wps, loss = 3.696\n",
      "[batch 546]: seen 547000 words at 6812 wps, loss = 3.694\n",
      "[batch 617]: seen 618000 words at 6836 wps, loss = 3.693\n",
      "[batch 688]: seen 689000 words at 6858 wps, loss = 3.689\n",
      "[batch 758]: seen 759000 words at 6864 wps, loss = 3.689\n",
      "[batch 827]: seen 828000 words at 6863 wps, loss = 3.688\n",
      "[batch 895]: seen 896000 words at 6857 wps, loss = 3.686\n",
      "[batch 964]: seen 965000 words at 6860 wps, loss = 3.684\n",
      "[batch 1032]: seen 1033000 words at 6855 wps, loss = 3.684\n",
      "[batch 1102]: seen 1103000 words at 6862 wps, loss = 3.683\n",
      "[batch 1172]: seen 1173000 words at 6869 wps, loss = 3.679\n",
      "[batch 1241]: seen 1242000 words at 6867 wps, loss = 3.680\n",
      "[batch 1309]: seen 1310000 words at 6861 wps, loss = 3.680\n",
      "[batch 1380]: seen 1381000 words at 6870 wps, loss = 3.680\n",
      "[batch 1448]: seen 1449000 words at 6864 wps, loss = 3.679\n",
      "[batch 1517]: seen 1518000 words at 6863 wps, loss = 3.677\n",
      "[batch 1585]: seen 1586000 words at 6860 wps, loss = 3.676\n",
      "[batch 1652]: seen 1653000 words at 6853 wps, loss = 3.675\n",
      "[batch 1721]: seen 1722000 words at 6854 wps, loss = 3.674\n",
      "[batch 1791]: seen 1792000 words at 6856 wps, loss = 3.673\n",
      "[batch 1860]: seen 1861000 words at 6857 wps, loss = 3.672\n",
      "[batch 1930]: seen 1931000 words at 6860 wps, loss = 3.672\n",
      "[batch 2000]: seen 2001000 words at 6864 wps, loss = 3.671\n",
      "[batch 2069]: seen 2070000 words at 6864 wps, loss = 3.670\n",
      "[batch 2139]: seen 2140000 words at 6865 wps, loss = 3.669\n",
      "[batch 2209]: seen 2210000 words at 6867 wps, loss = 3.668\n",
      "[epoch 4] Completed in 0:05:27\n",
      "[epoch 4] Train set: avg. loss: 4.814  (perplexity: 123.24)\n",
      "[epoch 4] Test set: avg. loss: 4.822  (perplexity: 124.25)\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "[batch 67]: seen 68000 words at 6780 wps, loss = 3.630\n",
      "[batch 137]: seen 138000 words at 6864 wps, loss = 3.620\n",
      "[batch 205]: seen 206000 words at 6809 wps, loss = 3.627\n",
      "[batch 274]: seen 275000 words at 6817 wps, loss = 3.621\n",
      "[batch 344]: seen 345000 words at 6840 wps, loss = 3.616\n",
      "[batch 414]: seen 415000 words at 6854 wps, loss = 3.620\n",
      "[batch 483]: seen 484000 words at 6858 wps, loss = 3.621\n",
      "[batch 553]: seen 554000 words at 6865 wps, loss = 3.619\n",
      "[batch 622]: seen 623000 words at 6860 wps, loss = 3.618\n",
      "[batch 691]: seen 692000 words at 6861 wps, loss = 3.615\n",
      "[batch 761]: seen 762000 words at 6871 wps, loss = 3.615\n",
      "[batch 830]: seen 831000 words at 6869 wps, loss = 3.615\n",
      "[batch 900]: seen 901000 words at 6876 wps, loss = 3.613\n",
      "[batch 970]: seen 971000 words at 6882 wps, loss = 3.612\n",
      "[batch 1040]: seen 1041000 words at 6885 wps, loss = 3.612\n",
      "[batch 1109]: seen 1110000 words at 6883 wps, loss = 3.610\n",
      "[batch 1179]: seen 1180000 words at 6886 wps, loss = 3.608\n",
      "[batch 1248]: seen 1249000 words at 6884 wps, loss = 3.608\n",
      "[batch 1318]: seen 1319000 words at 6889 wps, loss = 3.609\n",
      "[batch 1386]: seen 1387000 words at 6880 wps, loss = 3.609\n",
      "[batch 1455]: seen 1456000 words at 6879 wps, loss = 3.608\n",
      "[batch 1525]: seen 1526000 words at 6881 wps, loss = 3.607\n",
      "[batch 1594]: seen 1595000 words at 6878 wps, loss = 3.606\n",
      "[batch 1663]: seen 1664000 words at 6877 wps, loss = 3.605\n",
      "[batch 1732]: seen 1733000 words at 6878 wps, loss = 3.604\n",
      "[batch 1801]: seen 1802000 words at 6877 wps, loss = 3.603\n",
      "[batch 1868]: seen 1869000 words at 6868 wps, loss = 3.603\n",
      "[batch 1938]: seen 1939000 words at 6871 wps, loss = 3.603\n",
      "[batch 2007]: seen 2008000 words at 6870 wps, loss = 3.603\n",
      "[batch 2076]: seen 2077000 words at 6870 wps, loss = 3.602\n",
      "[batch 2146]: seen 2147000 words at 6871 wps, loss = 3.601\n",
      "[batch 2214]: seen 2215000 words at 6869 wps, loss = 3.600\n",
      "[epoch 5] Completed in 0:05:26\n",
      "[epoch 5] Train set: avg. loss: 4.750  (perplexity: 115.56)\n",
      "[epoch 5] Test set: avg. loss: 4.760  (perplexity: 116.71)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.batch_generator(train_ids, batch_size, max_time)\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        \n",
    "        run_epoch(lm, session, bi,\n",
    "              train=True, verbose=True,\n",
    "              tick_s=10, learning_rate=0.1)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print \"\"\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "        saver.restore(session, trained_filename)\n",
    "\n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], unicode):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print \"\\\"%s\\\" : %.02f\" % (\" \".join(words), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_saved/rnnlm_trained\n",
      "\"i shall be miserable if i have not an excellent library.\" : -4.91\n",
      "\"to be fond of dancing was a certain step towards falling in love.\" : -6.22\n",
      "\"don't let the door hit you on your way out.\" : -5.82\n",
      "\"he was totally into it.\" : -5.13\n",
      "\"to be or not to be.\" : -4.63\n",
      "\"come on y'all, don't be like that!\" : -5.00\n",
      "\"get outta here\" : -6.03\n"
     ]
    }
   ],
   "source": [
    "sents = [\"i shall be miserable if i have not an excellent library.\",\n",
    "         \"to be fond of dancing was a certain step towards falling in love.\",\n",
    "         \"don't let the door hit you on your way out.\",\n",
    "         \"he was totally into it.\",\n",
    "         \"to be or not to be.\",\n",
    "         \"come on y'all, don't be like that!\",\n",
    "         \"get outta here\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
