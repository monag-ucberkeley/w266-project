{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, csv, copy, random, unittest\n",
    "import time\n",
    "import itertools, collections\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "\n",
    "import nltk\n",
    "\n",
    "from shared_lib import utils, vocabulary, ngram_lm, ngram_utils\n",
    "\n",
    "random.seed(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Read TSV tokens from files generated by Book-NLP into a pandas dataframe.\\n2. Read tokens for each sentence.\\n3. Pad the sentence with two start tags for Trigram Model.\\n4. Canonicalize the words - util.py for vocabulary\\n5. End the sentence with a closing sentence tag.\\n6. Split the dataset into train and test\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. Read TSV tokens from files generated by Book-NLP into a pandas dataframe.\n",
    "2. Read tokens for each sentence.\n",
    "3. Pad the sentence with two start tags for Trigram Model.\n",
    "4. Canonicalize the words - util.py for vocabulary\n",
    "5. End the sentence with a closing sentence tag.\n",
    "6. Split the dataset into train and test\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books tokenized:  2998\n",
      "A few titles:  Grant_Allen___Science_in_Arcady.tokens\n",
      "Zane_Grey___The_Last_of_the_Plainsmen.tokens\n",
      "Edward_Phillips_Oppenheim___Anna_the_Adventuress.tokens\n",
      "Anthony_Trollope___The_Chateau_of_Prince_Polignac.tokens\n",
      "Benjamin_Disraeli___The_Voyage_of_Captain_Popanilla.tokens\n",
      "Henry_David_Thoreau___Canoeing_in_the_wilderness.tokens\n",
      "Edward_Stratemeyer___The_Rover_Boys_in_Southern_Waters.tokens\n"
     ]
    }
   ],
   "source": [
    "indir = '../book-nlp-master/data/tokens.gutenberg'\n",
    "books = []\n",
    "for root, dirs, filenames in os.walk(indir):\n",
    "    for f in filenames:\n",
    "         books.append(f)\n",
    "print \"Number of books tokenized: \", len(books)\n",
    "print \"A few titles: \", \n",
    "print books[3]\n",
    "print books[10]\n",
    "print books[102]\n",
    "print books[305]\n",
    "print books[340]\n",
    "print books[500]\n",
    "print books[654]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing book:  0 Abraham_Lincoln___Lincoln's_First_Inaugural_Address.tokens\n",
      "Processing book:  1 Abraham_Lincoln___Lincoln's_Gettysburg_Address,_given_November_19,_1863.tokens\n",
      "Processing book:  2 Abraham_Lincoln___Lincoln's_Inaugurals,_Addresses_and_Letters_(Selections).tokens\n",
      "Processing book:  3 Abraham_Lincoln___Lincoln's_Second_Inaugural_Address.tokens\n",
      "Processing book:  4 Abraham_Lincoln___Lincoln_Letters.tokens\n",
      "Processing book:  5 Abraham_Lincoln___Speeches_and_Letters_of_Abraham_Lincoln,_1832-1865.tokens\n",
      "Processing book:  6 Abraham_Lincoln___State_of_the_Union_Addresses.tokens\n",
      "Processing book:  7 Abraham_Lincoln___The_Emancipation_Proclamation.tokens\n",
      "Processing book:  8 Abraham_Lincoln___The_Life_and_Public_Service_of_General_Zachary_Taylor:_An_Address.tokens\n",
      "Processing book:  9 Abraham_Lincoln___The_Writings_of_Abraham_Lincoln,_Volume_1:_1832-1843.tokens\n",
      "Processing book:  10 Abraham_Lincoln___The_Writings_of_Abraham_Lincoln,_Volume_2:_1843-1858.tokens\n",
      "Processing book:  11 Abraham_Lincoln___The_Writings_of_Abraham_Lincoln,_Volume_3.tokens\n",
      "Processing book:  12 Abraham_Lincoln___The_Writings_of_Abraham_Lincoln,_Volume_4.tokens\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "canonicalized_words = []\n",
    "V = 20681\n",
    "# tempBooks = [books[3], books[5]]\n",
    "# print tempBooks\n",
    "\n",
    "start = time.time()\n",
    "for book_num, book in enumerate(sorted(books)): \n",
    "    if book.startswith('Abraham_Lincoln_'):\n",
    "        print \"Processing book: \", book_num, book\n",
    "        df = pd.read_csv('../book-nlp-master/data/tokens.gutenberg/'+ book, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for i in xrange(df[\"sentenceID\"].max()):\n",
    "            tokens.append('<s>')\n",
    "            tokens.append('<s>')\n",
    "            for word in df.query('sentenceID == '+ str(i) + ' & deprel != \"punct\"')['originalWord']:\n",
    "                if type(word) != str:\n",
    "                    word = str(word)\n",
    "                word = word.decode('ascii', 'ignore')\n",
    "                tokens.append(word)\n",
    "                canonicalized_words.append(utils.canonicalize_word(word))\n",
    "            tokens.append('</s>')\n",
    "\n",
    "\n",
    "tokens_length = len(tokens)\n",
    "\n",
    "print \"tokens_length - \", tokens_length\n",
    "\n",
    "vocab = vocabulary.Vocabulary(canonicalized_words, size=V)\n",
    "train_tokens  = tokens[ : int(tokens_length * 0.8)]\n",
    "test_tokens = tokens[int(tokens_length * 0.8) : ]\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print \"Train set vocabulary: %d words\" % vocab.size\n",
    "print \"Train set tokens: %d \" % len(train_tokens)\n",
    "print \"Test set tokens: %d \" % len(test_tokens)\n",
    "print \"First 10 Train Tokens: \", train_tokens[0:10]\n",
    "print \"First 10 Test Tokens: \", test_tokens[0:10]  \n",
    "\n",
    "print \"Time: {0}\".format(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20681"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stokens = set(tokens)\n",
    "len(stokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM... done in 3.59 s\n",
      "=== N-gram Language Model stats ===\n",
      "19049 unique 1-grams\n",
      "173304 unique 2-grams\n",
      "365207 unique 3-grams\n",
      "Optimal memory usage (counts only): 12 MB\n"
     ]
    }
   ],
   "source": [
    "reload(ngram_lm)\n",
    "Model = ngram_lm.KNTrigramLM\n",
    "t0 = time.time()\n",
    "print \"Building trigram LM...\",\n",
    "lm = Model(train_tokens)\n",
    "\n",
    "print \"done in %.02f s\" % (time.time() - t0)\n",
    "ngram_utils.print_stats(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> A. LINCOLN </s>\n",
      "[2 tokens; log P(seq): -4.86]\n",
      "\n",
      "<s> <s> Paper was such an event you say so and that the place on the same source as reported in said\n",
      "[20 tokens; log P(seq): -85.91]\n",
      "\n",
      "<s> <s> I add that I now do with them </s>\n",
      "[8 tokens; log P(seq): -41.31]\n",
      "\n",
      "<s> <s> Let them beware of the States and people will have extinguished slavery in these resolutions read from MS. but with\n",
      "[20 tokens; log P(seq): -90.50]\n",
      "\n",
      "<s> <s> There is understood that on receiving this government was against it </s>\n",
      "[11 tokens; log P(seq): -62.94]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\", \"<s>\"]\n",
    "    for i in range(max_length):\n",
    "        seq.append(ngram_utils.predict_next(lm, seq))\n",
    "        # Stop at end-of-sentence.\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print \" \".join(seq)\n",
    "    print \"[{1:d} tokens; log P(seq): {0:.02f}]\".format(*ngram_utils.score_seq(lm, seq))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 12.12\n",
      "Number of test tokens:  164051\n",
      "Test perplexity: 67.54\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, train_tokens)\n",
    "print \"Train perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))\n",
    "print \"Number of test tokens: \", len(test_tokens)\n",
    "\n",
    "lm.set_live_params(k = 0.001, delta=0.3)\n",
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, test_tokens)\n",
    "print \"Test perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing book:  George_Alfred_Henty___March_to_Magdala.tokens.p\n",
      "Processing book:  Baronness_Orczy___\"Unto_Caesar\".tokens.p\n",
      "Processing book:  Henry_Rider_Haggard___Moon_of_Israel.tokens.p\n",
      "Processing book:  Sir_Walter_Scott___Quentin_Durward.tokens.p\n",
      "Processing book:  Sir_Walter_Scott___The_Black_Dwarf.tokens.p\n",
      "Processing book:  George_Bernard_Shaw___Cashel_Byron's_Profession.tokens.p\n",
      "Processing book:  R_M_Ballantyne___The_Butterfly's_Ball.tokens.p\n",
      "Processing book:  Oscar_Wilde___The_Canterville_Ghost.tokens.p\n",
      "Processing book:  Herman_Melville___The_Confidence-Man.tokens.p\n",
      "Processing book:  James_Otis___Defending_the_Island.tokens.p\n",
      "Processing book:  Grant_Allen___Science_in_Arcady.tokens.p\n",
      "Processing book:  William_Dean_Howells___A_Chance_Acquaintance.tokens.p\n",
      "Processing book:  Bertrand_Russell___Our_Knowledge_of_the_External_World_as_a_Field_for_Scientific_Method_in_Philosophy.tokens.p\n",
      "Processing book:  Ambrose_Bierce___The_Collected_Works_of_Ambrose_Bierce,_Volume_8.tokens.p\n",
      "Processing book:  Sir_Walter_Scott___The_Talisman.tokens.p\n",
      "Processing book:  William_Blake___Songs_of_Innocence_and_Songs_of_Experience.tokens.p\n",
      "Processing book:  Lucy_Maud_Montgomery___Rainbow_Valley.tokens.p\n",
      "Processing book:  Daniel_Defoe___A_New_Voyage_Round_the_World_by_a_Course_Never_Sailed_Before.tokens.p\n",
      "Processing book:  Robert_Louis_Stevenson___Island_Nights'_Entertainments.tokens.p\n",
      "Processing book:  Nathaniel_Hawthorne___The_Intelligence_Office_(From_\"Mosses_From_An_Old_Manse\").tokens.p\n",
      "Processing book:  Zane_Grey___The_Heritage_of_the_Desert.tokens.p\n",
      "Processing book:  John_Bunyan___Miscellaneous_Pieces.tokens.p\n",
      "Processing book:  William_Dean_Howells___The_Man_of_Letters_as_a_Man_of_Business.tokens.p\n",
      "Processing book:  canonical_words.p\n",
      "Processing book:  Hamlin_Garland___The_Moccasin_Ranch.tokens.p\n",
      "Processing book:  Joseph_Conrad___Falk.tokens.p\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bb9f4d384f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanonicalized_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "canonical_words = []\n",
    "indir = 'processed/'\n",
    "books = []\n",
    "for root, dirs, filenames in os.walk(indir):\n",
    "    for filename in filenames:\n",
    "        print 'Processing book: ', filename\n",
    "        if filename == 'canonical_words.p':\n",
    "            with open(indir + filename, 'rb') as f:\n",
    "                canonical_words = pickle.load(f)\n",
    "        else:\n",
    "            with open(indir + filename, 'rb') as f:\n",
    "                tokens.append(pickle.load(f))\n",
    "                \n",
    "vocab = vocabulary.Vocabulary(canonicalized_words, size=V)\n",
    "train_tokens  = tokens[ : int(tokens_length * 0.8)]\n",
    "test_tokens = tokens[int(tokens_length * 0.8) : ]            \n",
    "\n",
    "print \"Train set vocabulary: %d words\" % vocab.size\n",
    "print \"Train set tokens: %d \" % len(train_tokens)\n",
    "print \"Test set tokens: %d \" % len(test_tokens)\n",
    "print \"First 10 Train Tokens: \", train_tokens[0:10]\n",
    "print \"First 10 Test Tokens: \", test_tokens[0:10]\n",
    "\n",
    "# with open('processed/'+book+'.p', 'a') as f:\n",
    "#     pickle.dump(tokens, f)\n",
    "# with open('processed/canonical_words.p', 'a') as c:\n",
    "#     pickle.dump(canonicalized_words, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'abcde'\n",
    "a.startswith('abc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
