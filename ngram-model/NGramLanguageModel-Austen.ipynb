{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First start with Jane Austen books tokenized. \n",
    "- Using original word at this point.\n",
    "- N-gram window needs to through one column across n-rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, csv, copy, random, unittest\n",
    "import itertools, collections\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import nltk\n",
    "\n",
    "from shared_lib import utils, vocabulary, ngram_lm, ngram_utils\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "austen_emma_all = []\n",
    "with open('../book-nlp-master/data/tokens.gutenberg/austen.pride.tokens', 'rb') as f:\n",
    "    reader = csv.reader(f, dialect=\"excel-tab\")\n",
    "    austen_emma_all = list(reader)\n",
    "# Remove header\n",
    "austen_emma_all = austen_emma_all[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'characterId'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-efe47bd8cf63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../book-nlp-master/data/tokens.gutenberg/Henry_James___The_Ambassadors.tokens'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_NONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'characterId > -1 & ner == \"PERSON\" & inQuotation == False'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentenceID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'characterId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# characters = {}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# for row in df.query('characterId > -1 & ner == \"PERSON\" & inQuotation == False'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sharmila_velamur/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sharmila_velamur/anaconda2/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2167\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2168\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2169\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3557)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3240)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4363)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'characterId'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../book-nlp-master/data/tokens.gutenberg/Henry_James___The_Ambassadors.tokens', sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "df.query('characterId > -1 & ner == \"PERSON\" & inQuotation == False')['sentenceID']['characterId']\n",
    "# characters = {}\n",
    "# for row in df.query('characterId > -1 & ner == \"PERSON\" & inQuotation == False'):\n",
    "#     print type(row)\n",
    "#     characters['id'] = row['characterId']\n",
    "#     if characters['id']['dialogs'] is None:\n",
    "#         characters['id']['dialogs'] = []\n",
    "#     characters['id']['dialogs'].append(' '.join(df.query('sentenceID == ' + row['sentenceID'])['originalWord']))\n",
    "# characters\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And what IS beastly , at all events , \" he added , \" is losing you . \"'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(df.query('sentenceID == 86')['originalWord'])\n",
    "' '.join(df.query('sentenceID == 5748')['originalWord'])\n",
    "' '.join(df.query('sentenceID == 99')['originalWord'])\n",
    "' '.join(df.query('sentenceID == 9116')['originalWord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 tokens from Jane Austen's Emma:\n",
      "Produced, by, An, Anonymous, Volunteer, EMMA, By, Jane, Austen, VOLUME, I, CHAPTER, I, Emma, Woodhouse, ,, handsome, ,, clever, ,, and, rich, ,, with, a, comfortable, home, and, happy, disposition\n"
     ]
    }
   ],
   "source": [
    "austen_emma_tokens = [line[7] for line in austen_emma_all]\n",
    "print 'The first 10 tokens from Jane Austen\\'s Emma:'\n",
    "print ', '.join(austen_emma_tokens[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle when needed\n",
    "random.shuffle(austen_emma_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154540.0\n",
      "Train set vocabulary: 6689 words\n",
      "Train set tokens: 154540 \n",
      "Test set tokens: 154540 \n",
      "First 10 Train Tokens:  ['Produced', 'by', 'An', 'Anonymous', 'Volunteer', 'EMMA', 'By', 'Jane', 'Austen', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',']\n",
      "First 10 Test Tokens:  ['were', 'thus', 'comfortably', 'occupied', ',', 'Mr.', 'Woodhouse', 'was', 'enjoying', 'a', 'full', 'flow', 'of', 'happy', 'regrets', 'and', 'fearful', 'affection', 'with', 'his']\n"
     ]
    }
   ],
   "source": [
    "tokens_length = len(austen_emma_tokens)\n",
    "print tokens_length * 0.8\n",
    "V = 40000\n",
    "train_tokens  = austen_emma_tokens[ : int(tokens_length * 0.8)]\n",
    "test_tokens = austen_emma_tokens[int(tokens_length * 0.2) : ]\n",
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in train_tokens), size=V)\n",
    "print \"Train set vocabulary: %d words\" % vocab.size\n",
    "print \"Train set tokens: %d \" % len(train_tokens)\n",
    "print \"Test set tokens: %d \" % len(test_tokens)\n",
    "print \"First 10 Train Tokens: \", train_tokens[0:20]\n",
    "print \"First 10 Test Tokens: \", test_tokens[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM... done in 0.87 s\n",
      "=== N-gram Language Model stats ===\n",
      "7080 unique 1-grams\n",
      "54967 unique 2-grams\n",
      "113902 unique 3-grams\n",
      "Optimal memory usage (counts only): 3 MB\n"
     ]
    }
   ],
   "source": [
    "Model = ngram_lm.KNTrigramLM\n",
    "t0 = time.time()\n",
    "print \"Building trigram LM...\",\n",
    "lm = Model(train_tokens)\n",
    "print \"done in %.02f s\" % (time.time() - t0)\n",
    "ngram_utils.print_stats(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> Highbury world ; “ circumstances absence John but thinking I my , not a only but days 10,000 am deal\n",
      "[20 tokens; log P(seq): -183.12]\n",
      "\n",
      "<s> <s> obliging own any sorry every ’s ” as but it -- . nothing a it had ” us allow _\n",
      "[20 tokens; log P(seq): -139.10]\n",
      "\n",
      "<s> <s> is much to state like carriage well dear of . was understand , rendered prevent , they as must any\n",
      "[20 tokens; log P(seq): -145.86]\n",
      "\n",
      "<s> <s> any ? the a match the had he , of and who find make in Emma as He very been\n",
      "[20 tokens; log P(seq): -149.63]\n",
      "\n",
      "<s> <s> little better and himself London I Mr. came Elton ought am was so indulge , be and a ordinarily risking\n",
      "[20 tokens; log P(seq): -155.35]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\", \"<s>\"]\n",
    "    for i in range(max_length):\n",
    "        seq.append(ngram_utils.predict_next(lm, seq))\n",
    "        # Stop at end-of-sentence.\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print \" \".join(seq)\n",
    "    print \"[{1:d} tokens; log P(seq): {0:.02f}]\".format(*ngram_utils.score_seq(lm, seq))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm.set_live_params(k = 0.001, delta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 11.01\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, train_tokens)\n",
    "print \"Train perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154540"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity: 29.13\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, test_tokens)\n",
    "print \"Test perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
