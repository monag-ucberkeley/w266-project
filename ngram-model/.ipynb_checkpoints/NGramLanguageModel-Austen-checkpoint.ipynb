{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First start with Jane Austen books tokenized. \n",
    "- Using original word at this point.\n",
    "- N-gram window needs to through one column across n-rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, csv, copy, random, unittest\n",
    "import itertools, collections\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import nltk\n",
    "\n",
    "from shared_lib import utils, vocabulary, ngram_lm, ngram_utils\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "austen_emma_all = []\n",
    "with open('../book-nlp-master/data/tokens/austen.emma.tokens', 'rb') as f:\n",
    "    reader = csv.reader(f, dialect=\"excel-tab\")\n",
    "    austen_emma_all = list(reader)\n",
    "# Remove header\n",
    "austen_emma_all = austen_emma_all[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 tokens from Jane Austen's Emma:\n",
      "Produced, by, An, Anonymous, Volunteer, EMMA, By, Jane, Austen, VOLUME, I, CHAPTER, I, Emma, Woodhouse, ,, handsome, ,, clever, ,, and, rich, ,, with, a, comfortable, home, and, happy, disposition\n"
     ]
    }
   ],
   "source": [
    "austen_emma_tokens = [line[7] for line in austen_emma_all]\n",
    "print 'The first 10 tokens from Jane Austen\\'s Emma:'\n",
    "print ', '.join(austen_emma_tokens[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle when needed\n",
    "random.shuffle(austen_emma_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154540.0\n",
      "Train set vocabulary: 6704 words\n",
      "Train set tokens: 154540 \n",
      "Test set tokens: 154540 \n",
      "First 10 Train Tokens:  [',', ',', 'greater', 'the', 'that', 'obliged', 'with', 'properly', 'spot', 'obliging', 'but', 'not', 'furniture', 'Mr.', 'Harriet', ',', 'I', 'moment', 'as', 'for']\n",
      "First 10 Test Tokens:  ['too', 'frightened', 'Miss', 'disagree', 'ceremony', 'I', 'Mrs.', 'impossible', 'such', 'much', 'I', 'of', 'did', 'were', 'a', 'you', 'her', 'in', 'I', 'say']\n"
     ]
    }
   ],
   "source": [
    "tokens_length = len(austen_emma_tokens)\n",
    "print tokens_length * 0.8\n",
    "V = 40000\n",
    "train_tokens  = austen_emma_tokens[ : int(tokens_length * 0.8)]\n",
    "test_tokens = austen_emma_tokens[int(tokens_length * 0.2) : ]\n",
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in train_tokens), size=V)\n",
    "print \"Train set vocabulary: %d words\" % vocab.size\n",
    "print \"Train set tokens: %d \" % len(train_tokens)\n",
    "print \"Test set tokens: %d \" % len(test_tokens)\n",
    "print \"First 10 Train Tokens: \", train_tokens[0:20]\n",
    "print \"First 10 Test Tokens: \", test_tokens[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM... done in 1.21 s\n",
      "=== N-gram Language Model stats ===\n",
      "7096 unique 1-grams\n",
      "83823 unique 2-grams\n",
      "147058 unique 3-grams\n",
      "Optimal memory usage (counts only): 5 MB\n"
     ]
    }
   ],
   "source": [
    "Model = ngram_lm.KNTrigramLM\n",
    "t0 = time.time()\n",
    "print \"Building trigram LM...\",\n",
    "lm = Model(train_tokens)\n",
    "print \"done in %.02f s\" % (time.time() - t0)\n",
    "ngram_utils.print_stats(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> Highbury world ; “ circumstances absence John but thinking I my , not a only but days 10,000 am deal\n",
      "[20 tokens; log P(seq): -183.12]\n",
      "\n",
      "<s> <s> obliging own any sorry every ’s ” as but it -- . nothing a it had ” us allow _\n",
      "[20 tokens; log P(seq): -139.10]\n",
      "\n",
      "<s> <s> is much to state like carriage well dear of . was understand , rendered prevent , they as must any\n",
      "[20 tokens; log P(seq): -145.86]\n",
      "\n",
      "<s> <s> any ? the a match the had he , of and who find make in Emma as He very been\n",
      "[20 tokens; log P(seq): -149.63]\n",
      "\n",
      "<s> <s> little better and himself London I Mr. came Elton ought am was so indulge , be and a ordinarily risking\n",
      "[20 tokens; log P(seq): -155.35]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\", \"<s>\"]\n",
    "    for i in range(max_length):\n",
    "        seq.append(ngram_utils.predict_next(lm, seq))\n",
    "        # Stop at end-of-sentence.\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print \" \".join(seq)\n",
    "    print \"[{1:d} tokens; log P(seq): {0:.02f}]\".format(*ngram_utils.score_seq(lm, seq))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm.set_live_params(k = 0.001, delta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 11.01\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, train_tokens)\n",
    "print \"Train perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154540"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity: 29.13\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, test_tokens)\n",
    "print \"Test perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
