{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: n-gram Language Modeling\n",
    "\n",
    "In this part of the assignment, we'll expand on the `SimpleTrigramLM` from the live session demo. We'll add smoothing to improve performance on unseen data, and explore some of the properties of the smoothed model.\n",
    "\n",
    "If you haven't looked over the simple trigram LM in [lm1.ipynb](../../materials/week2/lm1.ipynb), we recommend you look through it; this assignment will use a very similar setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, unittest\n",
    "import itertools, collections\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import ngram_lm, ngram_lm_test\n",
    "import ngram_utils\n",
    "from shared_lib import utils, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add-k Smoothing\n",
    "\n",
    "Recall our unsmoothed maximum likelihood estimate of $ P(w_i\\ |\\ w_{i-1}, w_{i-2})$ where we use the raw distribution over words seen in a context in the training data:\n",
    "\n",
    "$$  \\hat{P}(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{C_{abc}}{\\sum_{c'} C_{abc'}} $$\n",
    "\n",
    "Add-k smoothing is the simple refinement where we add $k > 0$ to each count $C_{abc}$, pretending we've seen every vocabulary word $k$ extra times in each context. So we have:\n",
    "\n",
    "$$ \\hat{P}_k(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{C_{abc} + k}{\\sum_{c'} (C_{abc'} + k)} = \\frac{C_{abc} + k}{C_{ab} + k\\cdot|V|} $$\n",
    "\n",
    "where $|V|$ is the size of our vocabulary.\n",
    "\n",
    "In the questions below and in the code, we'll refer to $(w_{i-2}, w_{i-1})$ as the *context*, and $w_i$ as the current *word*. By convention, we'll somewhat interchangeably refer to the sequence $(w_{i-2}, w_{i-1}, w)$ as $abc$.\n",
    "\n",
    "### Part (a): Short answer questions\n",
    "\n",
    "Give brief answers to the following, in the cell below.\n",
    "\n",
    "1. If we encounter a new context `(foo, bar)` unseen in the training data, what will the predicted *distribution* $\\hat{P}_k(w\\ |\\ \\text{foo}, \\text{bar})$ be? How does your answer depend on $k$?\n",
    "2. Is this a good estimate or can we do better?\n",
    "3. If we encounter a new word in a familiar context (i.e. `ab` is in the corpus, but `abq` is not), what will our predicted probability $\\hat{P}_k(q\\ |\\ b, a)$ be? Give your answer in terms of $C_{ab}$, $k$, and $|V|$.\n",
    "4. Based on your answer to question 3, in which context will your model predict a higher probability of *any* unknown word?  \n",
    "Context (a): `<s> the ___`  \n",
    "Context (b): `Mister Rogers ___`  \n",
    "Assume $C_{\\text{<s>, the}} = 10000$ and $C_{\\text{Mister, Rogers}} = 47$.\n",
    "5. Based on your knowledge of language, which of the contexts from question 4 *should* have a higher probability of an unknown word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers for Part (a)\n",
    "\n",
    "Please keep answers brief (1-2 lines).\n",
    "\n",
    "Hint: You can use LaTeX to typeset math, e.g. `$ f(x) = x^2 $` will render as $ f(x) = x^2 $.\n",
    "\n",
    "1. If foo and bar were never encountered in training, $C_{abc} = 0, C_{abc'}=0$. Then we get $$\\hat{P}_k(w_i = c\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{0 + k}{\\sum_{c'} (0 + k)} = \\frac{k}{k\\cdot|V|} =  \\frac{1}{|V|}$$ \n",
    "Thus the k value does not matter for unknown context. <br/><br/>\n",
    "2. This is better than the Laplace smoothing. But using characters in a word to infer probability is likely to yield better results than lumping all unencountered words into one single UNKNOWN class. <br/><br/>\n",
    "3. $$\\hat{P}_k(w_i = q\\ |\\ w_{i-1} = b, w_{i-2} = a) = \\frac{0 + k}{\\sum_{q'} (C_{abq'} + k)} = \\frac{k}{C_{ab}+k\\cdot|V|}$$ <br/><br/>\n",
    "4. Given $C_{\\text{<s>, the}} = 10000$ and $C_{\\text{Mister, Rogers}} = 47$, Context (b) will predict the next word higher probability ($\\frac{k}{47}$) than Context (a) ($\\frac{k}{10000}$)<br/><br/>\n",
    "5. Context (b) should have higher probability since the distribution of possible words after Mister Rogers is quite sharp; if you ask an American, they are very likely to predict the next word to be \"neighborhood\". There are too many possibilities for which word should come after \"the\".\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Implementing the Add-k Model\n",
    "\n",
    "Despite its shortcomings, it's worth implementing an add-k model as a baseline. Unlike the unsmoothed model, we'll be able to get some reasonable (or at least, finite) perplexity numbers which we can compare to the Kneser-Ney model below.\n",
    "\n",
    "We've provided some skeleton code (similar to [lm1.ipynb](../../materials/week2/lm1.ipynb)) in the `ngram_lm.py` file. In the `AddKTrigramLM` class, implement the following:\n",
    "- `__init__(self, words)`, which computes the necessary corpus statistics $C_{abc}$ and $C_{ab}$.\n",
    "- `next_word_proba(self, word, seq, k)`, which computes $\\hat{P}_k(w\\ |\\ w_{i-1}, w_{i-2})$\n",
    "\n",
    "See the function docstrings and in-line comments for more details. In particular, you may want to use `collections.defaultdict` and `dict.get()` to simplify handling of unknown keys. See [dict_notes.md](dict_notes.md) for a brief overview of how to use these.\n",
    "\n",
    "**Note on keys and word-order:** Convention in the math is to write context in reverse order, as in $P(w\\ |\\ w_{i-1}, w_{i-2})$, but in the code it'll be much easier to write things left-to-right as in normal English: so for the context \"`foo bar ___`\", you'll want to use `(\"foo\", \"bar\")` as a dict key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_context_totals (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_counts (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_next_word_proba_no_smoothing (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_no_mutate_on_predict (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "test_words (ngram_lm_test.TestAddKTrigramLM) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=5 errors=0 failures=0>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ngram_lm)\n",
    "reload(ngram_lm_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestAddKTrigramLM', ngram_lm_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kneser-Ney Smoothing\n",
    "\n",
    "In this part, we'll explore Kneser-Ney smoothing as a more sophisticated way of estimating unseen probabilities. \n",
    "\n",
    "When building an n-gram model, we're limited by the model order (e.g. trigram, 4-gram, or 5-gram) and how much data is available. Within that, we want to use as much information as possible. Within, say, a trigram context, we can compute a number of different statistics that might be helpful. Let's review a few goals:\n",
    "1. If we don't have good n-gram estimates, we want to back off to (n-1) grams.\n",
    "2. If we back off to (n-1) grams, we should do it \"smoothly\".\n",
    "3. Our counts $C_{abc}$ are probably _overestimates_ for the n-grams we observe (see *held-out reweighting*).\n",
    "4. Type fertilities tell us more about $P(w_{new}\\ |\\ \\text{context})$ than the unigram distribution does.\n",
    "\n",
    "Kneser-Ney smoothing combines all four of these ideas. \n",
    "\n",
    "**Absolute discounting** - which follows from 3. - gives us an easy way to backoff (1. and 2.), by distributing the subtracted probability mass among the backoff distribution $\\tilde{P}(c\\ |\\ b)$.  The amount to redistribute, $\\delta$, is a hyperparameter selected based on a cross-validation set in the usual way, although for this assignment we'll just let $\\delta = 0.75$.\n",
    "\n",
    "$$ P_{ad}(c\\ |\\ b, a) = \\frac{C_{abc} - \\delta}{C_{ab}} + \\alpha_{ab} \\tilde{P}(c\\ |\\  b) $$\n",
    "\n",
    "Where $\\alpha_{ab}$ is a backoff factor, derived from the counts, that guarantees that the probabilities are normalized: $\\sum_{c'} P_{ad}(c'\\ |\\ b, a) = 1$. This definition is recursive: if we let $\\tilde{P}(c\\ |\\  b) = P_{ad}(c\\ |\\ b)$, then the backoff distribution can also back off to even lower n-grams.\n",
    "\n",
    "*Note:* we need the numerator above to positive, so it should actually read $\\max(0, C_{abc} - \\delta)$.\n",
    "\n",
    "**Type fertility** is item 4. Instead of falling back to the unigram distribution at the end, we'll define $\\hat{P}(w)$ as proportional to the type fertility of $w$, or the *number of unique preceding words* $w_{i-1}$.  In the following equation, the word we are estimating the probability of is $c$.  $b'$ are the set of words we've found occurring before $c$ in the training data.\n",
    "\n",
    "$$ \\hat{P}_{tf}(c) \\propto \\left|\\ b' : C_{b'c} > 0\\ \\right| = tf(c)$$\n",
    "\n",
    "In order to make this a valid probability distribution, we need to normalize it with a factor $Z_{tf} = \\sum_{w} tf(w)$, so we have $\\hat{P}_{tf}(w) = \\frac{tf(w)}{Z_{tf}} $\n",
    "\n",
    "### KN Equations\n",
    "\n",
    "Putting it all together, we have our equations for a KN trigram model:\n",
    "\n",
    "$$ P_{kn}(c\\ |\\ b, a) = \\frac{\\max(0, C_{abc} - \\delta)}{C_{ab}} + \\alpha_{ab} P_{kn}(c\\ |\\  b) $$\n",
    "where the bigram backoff is:\n",
    "$$ P_{kn}(c\\ |\\ b) = \\frac{\\max(0, C_{bc} - \\delta)}{C_{b}} + \\alpha_{b} P_{kn}(c) $$\n",
    "and the unigram (type fertility) backoff is:\n",
    "$$ P_{kn}(c) = \\frac{tf(c)}{Z_{tf}} \\quad \\text{where} \\quad tf(c) = \\left|\\ b' : C_{b'c} > 0\\ \\right| $$\n",
    "\n",
    "Note that there is only one free parameter in this model, $\\delta$. You'll compute $\\alpha_{ab}$ and $\\alpha_{b}$ in the exercise below.\n",
    "\n",
    "### Part (c): Short answer questions\n",
    "\n",
    "Give brief answers to the following, in the cell below.\n",
    "\n",
    "1. Compute the value of $\\alpha_b$ such that $P_{kn}(c\\ |\\ b)$ is properly normalized. Give your answer in terms of the discount term $\\delta$, the context total $C_{b}$, and $\\text{nnz}(b) = \\left|\\ c' : C_{bc'} - \\delta > 0 \\ \\right|$, the number of bigrams `bc'` with positive (discounted) count.  \n",
    "**Hint:** solve $\\sum_{c'} P_{kn}(c'\\ |\\ b) = 1$ using the fact that $\\sum_{c'} P_{kn}(c') = 1$ and $\\sum_{c'} C_{bc'} = C_{b} $.  \n",
    "Note that if you replace $b$ with the context $ab$, you can re-use this result to define $\\alpha_{ab}$.  \n",
    "\n",
    "2. Based on your answer to question 1, in which case do you expect the KN model to rely more on the backoff distribution (i.e. higher $\\alpha$)? Assume $\\delta = 0.75$.   \n",
    "Case (a): context `<s> my name is ___`, which occurs 1000 times ending with 632 unique words.  \n",
    "Case (b): context `Mister Rogers ___`, which occurs 5 times and always ends with \"Neighborhood\".  \n",
    "Explain _briefly_ why this is the case, and why this is reasonable behavior given your intuition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers for Part (c)\n",
    "\n",
    "You can use LaTeX to typeset math, e.g. `$ f(x) = x^2 $` will render as $ f(x) = x^2 $.\n",
    "\n",
    "1. $$\\alpha_b = \\frac{\\delta}{C_b}  \\left|\\ c' : C_{bc'} - \\delta > 0 \\ \\right|$$\n",
    "<br/><br/>\n",
    "2. \n",
    "For Case (a), \n",
    "    $$\\alpha_b = \\frac{0.75}{1000}(632) = 0.474$$\n",
    "For Case (b), \n",
    "    $$\\alpha_b = \\frac{0.75}{5}(1) = 0.15$$\n",
    "Since the discounting is linearly proportional to the number of unique bigrams, it is not surprising that $\\alpha_b$ is higher in Case (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d): Implementing the KN Model\n",
    "\n",
    "Implement the `KNTrigramLM` in `ngram_lm.py`. As with the add-k model, we've provided some starter code for you; you need only fill in the marked blocks. `KNTrigramLM` also conforms to the exact same interface as the add-k model.\n",
    "\n",
    "You should:\n",
    "- Finish the implementation of `__init__(self, words)` to compute the necessary quantities\n",
    "- Implement the `kn_interp(...)` function, which interpolates between n-gram counts and a backoff distribution according to the absolute discounting rule (see definitions of $P_{kn}(c\\ |\\ a, b)$ and $P_{kn}(c\\ |\\ b)$). You'll need your definition of $\\alpha$ from (c).1. here.\n",
    "\n",
    "As before, see the function docstrings and in-line comments for more details.\n",
    "\n",
    "When you're done implementing it, run the cells below to train your model, sample some sentences, and evaluate your model on the dev set. Then jump to Part (e) for a few last questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_context_nnz (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_context_totals (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_counts (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_kn_interp (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_next_word_proba (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_no_mutate_on_predict (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_type_contexts (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_type_fertility (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_words (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "test_z_tf (ngram_lm_test.TestKNTrigramLM) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 10 tests in 0.014s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=10 errors=0 failures=0>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ngram_lm)\n",
    "reload(ngram_lm_test)\n",
    "unittest.TextTestRunner(verbosity=2).run(\n",
    "    unittest.TestLoader().loadTestsFromName(\n",
    "        'TestKNTrigramLM', ngram_lm_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training your Model\n",
    "\n",
    "The same code below can be used with either model; in the cell where it says \"Select your Model\", you can choose the add-k model or the KN model.\n",
    "\n",
    "## Loading & Preprocessing\n",
    "Once again, we'll build our model on the Brown corpus. We'll do an 80/20 train/test split, and preprocess words by lowercasing and replacing digits with `DG` (so `2016` becomes `DGDGDGDG`).\n",
    "\n",
    "In a slight departure from the `lm1.ipynb` demo, we'll restrict the vocabulary to 40000 words. This way, a small fraction of the *training* data will be mapped to `<unk>` tokens, and the model can learn n-gram probabilities that include `<unk>` for prediction on the test set. (If we interpret `<unk>` as meaning \"rare word\", then this is somewhat plausible as a way to infer things about the class of rare words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/sharmila_velamur/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Loaded 57340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45872 sentences (979646 tokens)\n",
      "Test set: 11468 sentences (181546 tokens)\n",
      "Train set vocabulary: 30000 words\n"
     ]
    }
   ],
   "source": [
    "assert(nltk.download('brown'))  # Make sure we have the data.\n",
    "corpus = nltk.corpus.brown\n",
    "V = 30000\n",
    "train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=False)\n",
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(train_sents)), size=V)\n",
    "# vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(corpus.sents())), size=V)\n",
    "print \"Train set vocabulary: %d words\" % vocab.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our smoothed models will also be trigram models, so for convenience we'll also prepend *two* `<s>` markers. (We could avoid this, but then we'd need special handling for the first token of each sentence.)\n",
    "\n",
    "To make it easier to work with, we'll take the list of tokens as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: \n",
      "array(['<s>', '<s>', u'the', u'fulton', u'county', u'grand', u'jury',\n",
      "       u'said', u'friday', u'an', u'investigation', u'of', u\"atlanta's\",\n",
      "       u'recent', u'primary', u'election', u'produced', u'``', u'no',\n",
      "       u'evidence'], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "def sents_to_tokens(sents):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with padding for a trigram model.\"\"\"\n",
    "    padded_sentences = ([\"<s>\", \"<s>\"] + s + [\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([utils.canonicalize_word(w, wordset=vocab.wordset) \n",
    "                     for w in utils.flatten(padded_sentences)], dtype=object)\n",
    "\n",
    "train_tokens = sents_to_tokens(train_sents)\n",
    "test_tokens = sents_to_tokens(test_sents)\n",
    "print \"Sample data: \\n\" + repr(train_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select your model\n",
    "\n",
    "Select either `AddKTrigramLM` or `KNTrigramLM` in the cell below. If switching models, you only need to re-run the cells below here - no need to re-run the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM... done in 8.35 s\n",
      "=== N-gram Language Model stats ===\n",
      "0 unique 1-grams\n",
      "0 unique 2-grams\n",
      "733388 unique 3-grams\n",
      "Optimal memory usage (counts only): 16 MB\n"
     ]
    }
   ],
   "source": [
    "import ngram_lm\n",
    "reload(ngram_lm)\n",
    "\n",
    "# Run Add-K Trigram\n",
    "Model = ngram_lm.AddKTrigramLM\n",
    "\n",
    "t0 = time.time()\n",
    "print \"Building trigram LM...\",\n",
    "lm = Model(train_tokens)\n",
    "print \"done in %.02f s\" % (time.time() - t0)\n",
    "ngram_utils.print_stats(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM... done in 8.51 s\n",
      "=== N-gram Language Model stats ===\n",
      "30000 unique 1-grams\n",
      "358274 unique 2-grams\n",
      "733388 unique 3-grams\n",
      "Optimal memory usage (counts only): 24 MB\n"
     ]
    }
   ],
   "source": [
    "import ngram_lm\n",
    "reload(ngram_lm)\n",
    "\n",
    "# RUN KN Trigram\n",
    "Model = ngram_lm.KNTrigramLM\n",
    "\n",
    "t0 = time.time()\n",
    "print \"Building trigram LM...\",\n",
    "lm = Model(train_tokens)\n",
    "print \"done in %.02f s\" % (time.time() - t0)\n",
    "ngram_utils.print_stats(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `params` to change the smoothing factor. `AddKTrigramLM` will ignore the value of `delta`, and `KNTrigramLM` will ignore `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm.set_live_params(k = 0.001, delta=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> the most illuminating corresponding cone reacted thrusting duplication connective combines nonracial sputniks having reese kyne seasonal ridge temptingly respiratory setsw\n",
      "[20 tokens; log P(seq): -271.84]\n",
      "\n",
      "<s> <s> any needy family stewardess outlined venture wasteland foggy witnessed diplomatic schaefer defines grisly doubtful disaffiliated bod's appraise colon overfill finances\n",
      "[20 tokens; log P(seq): -274.27]\n",
      "\n",
      "<s> <s> `` workmen differentiate abeyance ache notches pirates trucking psychological foreigner jewett intuitive berth slate disrobe interfaces miseries greenhouse mechanism civil\n",
      "[20 tokens; log P(seq): -283.26]\n",
      "\n",
      "<s> <s> not chariot rules stirling intramuscularly individual-contributor whisked dissimilar maget liking turnpike purest cauffman crystal prison triangle travelers disappears lunchtime slugged\n",
      "[20 tokens; log P(seq): -293.50]\n",
      "\n",
      "<s> <s> most liar compete waymouth drums anecdote shivering aloud shit auto-europe patrolmen danube giving hitler's obsesses unbroken chahar dissolve thills frieze\n",
      "[20 tokens; log P(seq): -293.57]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\", \"<s>\"]\n",
    "    for i in range(max_length):\n",
    "        seq.append(ngram_utils.predict_next(lm, seq))\n",
    "        # Stop at end-of-sentence.\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print \" \".join(seq)\n",
    "    print \"[{1:d} tokens; log P(seq): {0:.02f}]\".format(*ngram_utils.score_seq(lm, seq))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> in DGDGDGDG in the meantime blocks and she would have some of the reef . </s>\n",
      "[15 tokens; log P(seq): -87.23]\n",
      "\n",
      "<s> <s> even in nature than his face of the day per passenger , i asked them to the mechanical , use\n",
      "[20 tokens; log P(seq): -135.64]\n",
      "\n",
      "<s> <s> i have been developed ? ? </s>\n",
      "[6 tokens; log P(seq): -27.07]\n",
      "\n",
      "<s> <s> john a. notte , jr. , mrs. william austin are filled by the pleading know discontinued its previously from back\n",
      "[20 tokens; log P(seq): -131.92]\n",
      "\n",
      "<s> <s> the basis of this time the secret ballot . </s>\n",
      "[9 tokens; log P(seq): -56.40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\", \"<s>\"]\n",
    "    for i in range(max_length):\n",
    "        seq.append(ngram_utils.predict_next(lm, seq))\n",
    "        # Stop at end-of-sentence.\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print \" \".join(seq)\n",
    "    print \"[{1:d} tokens; log P(seq): {0:.02f}]\".format(*ngram_utils.score_seq(lm, seq))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring on Held-Out Data\n",
    "\n",
    "Your KN model should get a perplexity of around 280 on the dev set with $\\delta = 0.75$. The add-k smoothing model will perform... somewhat worse :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 43.13\n",
      "Test perplexity: 3652.42\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, train_tokens)\n",
    "print \"Train perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))\n",
    "\n",
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, test_tokens)\n",
    "print \"Test perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 17.17\n",
      "Test perplexity: 286.60\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, train_tokens)\n",
    "print \"Train perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))\n",
    "\n",
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, test_tokens)\n",
    "print \"Test perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (e): Additional Questions\n",
    "\n",
    "Answer the following in the cell below.\n",
    "\n",
    "1. What is the average number of times our model sees any particular trigram, averaged across the trigrams we observed at least once? How about averaged across *all possible* trigrams? (*Hint:* you don't need to write any code for this - it should be a quick calculation.)\n",
    "2. Based on your answer above, do you think that a 4-gram or 5-gram model would perform better than the trigram model on the 1 million word Brown corpus? How about on a 42-billion word Wikipedia corpus?\n",
    "3. Which model generates more \"realistic\" sentences - `AddKTrigramLM`, `KNTrigramLM`, or the unsmoothed `SimpleTrigramLM` from the demo notebook? Is this in-line with their perplexity on the dev set?\n",
    "\n",
    "*Note:* on Assignment 3, we'll implement a neural network model that avoids the sparsity problem altogether and can achieve significantly better generalization even on a small dataset like the Brown corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers for Part (e)\n",
    "\n",
    "1. <br/><br/>\n",
    "2. I don't think a 4-gram or 5-gram model would perform better on the Brown corpus compared to trigram. In fact due to sparsity issues, higher order n-grams may not perform better unless there is a lot of data available to train on. Thus a 4-gram or 5-gram model is likely to perform better than the trigram on the 42-billion word wikipedia corpus.<br/><br/>\n",
    "3. KNTrigramLM generates better sentences followed by AddKTrigram and SimpleTrigram. This is in line with the perplexity on the dev set as lower values of perplexity indicate a better probability distribution and hence, a better prediction.<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just for fun: Linguistic Curiosities\n",
    "\n",
    "You might have seen this floating around the internet:\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "Let's see if it holds true, statistically at least. Note that log probabilities are always negative, so the smaller magnitude is better. And remember the log scale: a difference of score of 8 units means one utterance is $2^8 = 256$ times more likely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_for_scoring(sentence):\n",
    "    # Pre-process words, replace anything the model doesn't know\n",
    "    # with <unk>\n",
    "    words = [utils.canonicalize_word(w, wordset=known_words)\n",
    "             for w in sentence]\n",
    "    # Pad sequence with start and end markers\n",
    "    return [\"<s>\", \"<s>\"] + words + [\"</s>\"]\n",
    "\n",
    "known_words = vocab.wordset\n",
    "s0 = preprocess_for_scoring(\"square green plastic toys\".split())\n",
    "s1 = preprocess_for_scoring(\"plastic green square toys\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"s0 score: %.02f\" % ngram_utils.score_seq(lm, s0)[0]\n",
    "print \"s1 score: %.02f\" % ngram_utils.score_seq(lm, s1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "results = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "    words = list(adjs) + [noun]\n",
    "    seq = preprocess_for_scoring(words)\n",
    "    score = ngram_utils.score_seq(lm, seq)\n",
    "    results.append((score[0], words))\n",
    "\n",
    "# Sort results\n",
    "for score, words in sorted(results, reverse=True):\n",
    "    print \"\\\"%s\\\" : %.02f\" % (\" \".join(words), score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
