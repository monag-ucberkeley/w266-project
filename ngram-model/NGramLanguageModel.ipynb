{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First start with Jane Austen books tokenized. \n",
    "- Using original word at this point.\n",
    "- N-gram window needs to through one column across n-rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, csv, copy, random, unittest\n",
    "import itertools, collections\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import nltk\n",
    "\n",
    "from shared_lib import utils, vocabulary, ngram_lm, ngram_utils\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "austen_emma_all = []\n",
    "with open('../book-nlp-master/data/tokens/austen.emma.tokens', 'rb') as f:\n",
    "    reader = csv.reader(f, dialect=\"excel-tab\")\n",
    "    austen_emma_all = list(reader)\n",
    "# Remove header\n",
    "austen_emma_all = austen_emma_all[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 tokens from Jane Austen's Emma:\n",
      "Produced, by, An, Anonymous, Volunteer, EMMA, By, Jane, Austen, VOLUME\n"
     ]
    }
   ],
   "source": [
    "austen_emma_tokens = [line[7] for line in austen_emma_all]\n",
    "print 'The first 10 tokens from Jane Austen\\'s Emma:'\n",
    "print ', '.join(austen_emma_tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle when needed\n",
    "random.shuffle(austen_emma_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154540.0\n",
      "Train set vocabulary: 6684 words\n",
      "Train set tokens: 154540 \n",
      "Test set tokens: 38635 \n",
      "First 10 Train Tokens:  ['\\xe2\\x80\\x9c', 'with', 'her', 'be', 'dissolved', 'the', 'had', ',', 'well', 'indeed']\n",
      "First 10 Test Tokens:  ['not', 'whose', 'Mrs.', 'Emma', 'to', 'very', 'they', 'for', 'union', 'my']\n"
     ]
    }
   ],
   "source": [
    "tokens_length = len(austen_emma_tokens)\n",
    "print tokens_length * 0.8\n",
    "V = 30000\n",
    "train_tokens  = austen_emma_tokens[ : int(tokens_length * 0.8)]\n",
    "test_tokens = austen_emma_tokens[int(tokens_length * 0.8) : ]\n",
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in train_tokens), size=V)\n",
    "print \"Train set vocabulary: %d words\" % vocab.size\n",
    "print \"Train set tokens: %d \" % len(train_tokens)\n",
    "print \"Test set tokens: %d \" % len(test_tokens)\n",
    "print \"First 10 Train Tokens: \", train_tokens[0:10]\n",
    "print \"First 10 Test Tokens: \", test_tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/sharmila_velamur/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "Loaded 98552 sentences (2.62178e+06 tokens)\n",
      "Training set: 78841 sentences (2108453 tokens)\n",
      "Test set: 19711 sentences (513332 tokens)\n",
      "Train set vocabulary: 10000 words\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Feeding original text instead of the tokens generated to see if this works immly with the code from assignment-2\n",
    "'''\n",
    "from nltk.corpus import gutenberg\n",
    "assert(nltk.download('gutenberg'))\n",
    "V = 10000\n",
    "corpus = nltk.corpus.gutenberg\n",
    "train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=False)\n",
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(train_sents)), size=V)\n",
    "# vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(corpus.sents())), size=V)\n",
    "print \"Train set vocabulary: %d words\" % vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: \n",
      "array(['<s>', '<s>', u'[', u'emma', u'by', u'jane', '<unk>', u'DGDGDGDG',\n",
      "       u']', '</s>', '<s>', '<s>', u'volume', u'i', '</s>', '<s>', '<s>',\n",
      "       u'chapter', u'i', '</s>'], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sents_to_tokens(sents):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with padding for a trigram model.\"\"\"\n",
    "    padded_sentences = ([\"<s>\", \"<s>\"] + s + [\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([utils.canonicalize_word(w, wordset=vocab.wordset) \n",
    "                     for w in utils.flatten(padded_sentences)], dtype=object)\n",
    "\n",
    "train_tokens = sents_to_tokens(train_sents)\n",
    "test_tokens = sents_to_tokens(test_sents)\n",
    "print \"Sample data: \\n\" + repr(train_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM... done in 10.84 s\n",
      "=== N-gram Language Model stats ===\n",
      "10000 unique 1-grams\n",
      "319307 unique 2-grams\n",
      "968394 unique 3-grams\n",
      "Optimal memory usage (counts only): 27 MB\n"
     ]
    }
   ],
   "source": [
    "Model = ngram_lm.KNTrigramLM\n",
    "t0 = time.time()\n",
    "print \"Building trigram LM...\",\n",
    "lm = Model(train_tokens)\n",
    "print \"done in %.02f s\" % (time.time() - t0)\n",
    "ngram_utils.print_stats(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> i can fainted proposed , and you know ,' said she , with an m , sometimes ; that it\n",
      "[20 tokens; log P(seq): -126.56]\n",
      "\n",
      "<s> <s> when you were so high !\" </s>\n",
      "[6 tokens; log P(seq): -32.81]\n",
      "\n",
      "<s> <s> \" perhaps there might be as though she did not intend to add to your pardon . </s>\n",
      "[17 tokens; log P(seq): -74.59]\n",
      "\n",
      "<s> <s> he had rather , the son of david . </s>\n",
      "[9 tokens; log P(seq): -42.06]\n",
      "\n",
      "<s> <s> \" but why must you , my statutes and judgments which i command thee this day . </s>\n",
      "[17 tokens; log P(seq): -52.59]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [\"<s>\", \"<s>\"]\n",
    "    for i in range(max_length):\n",
    "        seq.append(ngram_utils.predict_next(lm, seq))\n",
    "        # Stop at end-of-sentence.\n",
    "        if seq[-1] == \"</s>\": break\n",
    "    print \" \".join(seq)\n",
    "    print \"[{1:d} tokens; log P(seq): {0:.02f}]\".format(*ngram_utils.score_seq(lm, seq))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm.set_live_params(k = 0.001, delta=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train perplexity: 19.55\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, train_tokens)\n",
    "print \"Train perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity: 302.38\n"
     ]
    }
   ],
   "source": [
    "log_p_data, num_real_tokens = ngram_utils.score_seq(lm, test_tokens)\n",
    "print \"Test perplexity: %.02f\" % (2**(-1*log_p_data/num_real_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
